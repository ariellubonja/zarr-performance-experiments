{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This Notebook uses Zarr's Partial Decompress feature. Zarr proposed another layer in their storage hierarchy with Sharding. See corresponding notebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a39fec7f8161d4d7"
  },
  {
   "cell_type": "markdown",
   "id": "35f3f7cc",
   "metadata": {},
   "source": [
    "## Found how to get partial_decompress=True from zarr_python library code, file \"build/lib/zarr/tests/test_core.py\" test function test_read_nitems_less_than_blocksize_from_multiple_chunks in line 2683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b19475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr # Check version\n",
    "zarr.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09861b38",
   "metadata": {},
   "source": [
    "Zarr Sharding pull request should show '0.0.0'\n",
    "\n",
    "If it says e.g. 2.13.2, then you're running on the \"proper\" published version (without Sharding support as of 28-Sept-2022)\n",
    "\n",
    "<font color=\"orange\">partial_decompress works on current version 2.13, no need for Sharding</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba2968",
   "metadata": {},
   "source": [
    "### 1. I wanna test partial_decompression by accessing a random index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "numpy_arr = np.empty((65536, 16384), dtype=np.int32) # ~3GB\n",
    "original_shape = numpy_arr.shape\n",
    "\n",
    "# 50k x 150k int32 is limit of M1 Pro 32GB\n",
    "for i in range(numpy_arr.shape[0]):\n",
    "#     numpy_arr[i][:] = list(range(original_shape[1]*i, original_shape[1]*(i+1)))\n",
    "    numpy_arr[i][:] = np.random.randint(low=0, high=original_shape[0], size=original_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a46315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memory used: \", numpy_arr.nbytes / 1024 / 1024 / 1024, \" GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e385d1-ea3e-494d-b39c-860329a66135",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_arr[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsstore = zarr.storage.FSStore('./fsstore256MBChunk')\n",
    "zarr_index_list = zarr.array(numpy_arr, store=fsstore, chunks=chunk_shape)\n",
    "zarr.save(fsstore, numpy_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ead9a",
   "metadata": {},
   "source": [
    "### Following code in zarr test_core.py:2653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628f002-450a-430c-99e5-b39576fd2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All these extra parameters are irrelevant to _partial_decompress being enabled. Path is enough\n",
    "# fsstore = zarr.storage.FSStore('./fsstore', key_separator=\"/\", auto_mkdir=True, mode='w', normalize_keys=False)\n",
    "\n",
    "fsstore = zarr.storage.FSStore('./fsstore4MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26dc7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size=(4096,4096)\n",
    "\n",
    "zarr_index_list = zarr.array(numpy_arr, store=fsstore)#, chunk_store = fsstore_chunk, partial_decompress=True, chunks=chunk_size, dtype='i4')# compressor=weak_compressor, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c241a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_index_list.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_partial_decompress: \", zarr_index_list._partial_decompress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c6700",
   "metadata": {},
   "source": [
    "<font color=\"orange\">As we can see, partial_decompress is False. Don't panic!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cf25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush to disk\n",
    "zarr.save(fsstore, numpy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "\n",
    "# read_only doesn't affect partial_decompress eligibility\n",
    "load_partial_decomp = zarr.Array(fsstore, read_only=True, partial_decompress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp._partial_decompress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372307f5",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\">Solved! Partial_decompress=True</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad449b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del numpy_arr # Don't need this anymore - save 3GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a93782",
   "metadata": {},
   "source": [
    "Sometimes chunks not even initialized? Sometimes 2 arrays are written to disk (i.e. a group). Not sure why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101cbae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp[0, 100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_disk_normally = zarr.open(fsstore) # , partial_decompress=True) -- This won't work !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa74e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_disk_normally._partial_decompress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09167752",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_disk_normally[0, 100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c806ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the two match?\n",
    "(load_partial_decomp[0, 100:105] == load_disk_normally[0, 100:105]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4fea5",
   "metadata": {},
   "source": [
    "### Let's test random access of chunks by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3741bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_shape = (2048, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b41cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices_list = np.random.randint(low=0, high=chunk_shape[0], size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk, Using partial decompression\n",
    "\n",
    "for index in rand_indices_list:\n",
    "    load_partial_decomp[0, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk, Not using partial decompression\n",
    "\n",
    "for index in rand_indices_list:\n",
    "    load_disk_normally[0, index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc6983",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\">Let's access the chunks using 2D accesses now</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd086c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices_tuples = (np.random.randint(low=0, high=chunk_shape[0], size=50), np.random.randint(low=0, high=chunk_shape[1], size=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22134b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices_tuples[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392ed49",
   "metadata": {},
   "source": [
    "TODO set range(50) programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a582a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk, Not using partial decompression\n",
    "\n",
    "for i in range(50):\n",
    "    load_partial_decomp[rand_indices_tuples[0][i], rand_indices_tuples[1][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk, Not using partial decompression\n",
    "\n",
    "for i in range(50):\n",
    "    load_disk_normally[rand_indices_tuples[0][i], rand_indices_tuples[1][i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c92308",
   "metadata": {},
   "source": [
    "## <font color=\"orange\"> New experiment: access one point per chunk in the whole array</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2558656",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "32**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af03c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fca90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rand_indices_whole_array = []\n",
    "\n",
    "xes = []\n",
    "ys = []\n",
    "\n",
    "for i in range(32): # there's 32x32 chunks. TODO get programmatically\n",
    "    # 2d point access - let's not just access first row of chunk\n",
    "    \n",
    "    # Random point across all chunks\n",
    "#     x = np.random.randint(low=i*chunk_shape[0]+5, high=(i+1)*chunk_shape[0] + 5)\n",
    "#     y = np.random.randint(low=i*chunk_shape[1]+5, high=(i+1)*chunk_shape[1]+5)\n",
    "    \n",
    "    xes.append(i*chunk_shape[0]+50)\n",
    "    ys.append(i*chunk_shape[1]+50)\n",
    "    \n",
    "    # \"Same\" point across all chunks\n",
    "\n",
    "#     rand_indices_whole_array.append((x,y))\n",
    "    \n",
    "    # 1D access - makes indexing complicated\n",
    "#     rand_indices_whole_array.append(np.random.randint(low=i*chunk_shape[0], high=(i+1)*chunk_shape[1]))\n",
    "\n",
    "rand_indices_whole_array = list(product(xes, ys))\n",
    "rand_indices_whole_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dddfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rand_indices_whole_array) # Should equal n_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae41da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from whole array, randomly 1 point from each chunk\n",
    "\n",
    "for tup in rand_indices_whole_array:\n",
    "    load_partial_decomp[tup[0], tup[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85978081",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from whole array, randomly 1 point from each chunk\n",
    "# TODO Don't know if my indexing is correct!!\n",
    "\n",
    "for tup in rand_indices_whole_array:\n",
    "    load_disk_normally[tup[0], tup[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup: \", 1.34 / .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d2efd",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Let's test advanced mask indexing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 0-1 boolean mask with size same as Whole array\n",
    "#    Don't know how to do just 1 chunk masking\n",
    "bool_mask = np.random.randint(low=0, high=2, size=original_shape, dtype=bool)\n",
    "bool_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Partial decompress\n",
    "\n",
    "# _ to prevent printing\n",
    "_ = load_partial_decomp.get_mask_selection(bool_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# NO Partial decompress\n",
    "\n",
    "# _ to prevent printing\n",
    "_ = load_disk_normally.get_mask_selection(bool_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f677c2a",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Trying 16MB chunks. partial_decompress Benefits should be bigger for bigger chunks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsstore_8mb = zarr.storage.FSStore(\"./fsstore16MBChunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ccdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_shape = (4096,1024) #16MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee67780",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_index_list_16MB_Chunk = zarr.array(numpy_arr, store=fsstore_8mb, chunks = chunk_size_8mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr.save(fsstore_16mb, zarr_index_list_8MB_Chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48257921",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp_16mb = zarr.Array(fsstore_16mb, partial_decompress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8daf8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_disk_normally_16mb = zarr.Array(fsstore_16mb, partial_decompress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices_tuples = (np.random.randint(low=0, high=chunk_shape[0], size=50), np.random.randint(low=0, high=chunk_shape[1], size=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk, Not using partial decompression\n",
    "\n",
    "for i in range(50):\n",
    "    load_partial_decomp_16mb[rand_indices_tuples[0][i], rand_indices_tuples[1][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk, Not using partial decompression\n",
    "\n",
    "for i in range(50):\n",
    "    load_disk_normally_16mb[rand_indices_tuples[0][i], rand_indices_tuples[1][i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd37f6f",
   "metadata": {},
   "source": [
    "## <font color=\"red\">64 MB chunks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsstore = zarr.storage.FSStore(\"./fsstore64MBChunk\")\n",
    "chunk_shape = (8192,2048)\n",
    "zarr_index_list = zarr.array(numpy_arr, store=fsstore, chunks = chunk_shape)\n",
    "zarr.save(fsstore, zarr_index_list)\n",
    "load_partial_decomp = zarr.Array(fsstore, partial_decompress=True)\n",
    "load_disk_normally = zarr.Array(fsstore, partial_decompress=False)\n",
    "rand_indices_tuples = (np.random.randint(low=0, high=chunk_shape[0], size=50), np.random.randint(low=0, high=chunk_shape[1], size=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d1e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk, Not using partial decompression\n",
    "\n",
    "for i in range(50):\n",
    "    load_partial_decomp[rand_indices_tuples[0][i], rand_indices_tuples[1][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk, Not using partial decompression\n",
    "\n",
    "for i in range(50):\n",
    "    load_disk_normally[rand_indices_tuples[0][i], rand_indices_tuples[1][i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feca4a0",
   "metadata": {},
   "source": [
    "## <font color=\"red\">256MB Chunk</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b29913",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsstore = zarr.storage.FSStore(\"./fsstore256MBChunk\")\n",
    "chunk_shape = (16384,4096)\n",
    "zarr_index_list = zarr.array(numpy_arr, store=fsstore, chunks = chunk_shape)\n",
    "zarr.save(fsstore, zarr_index_list)\n",
    "load_partial_decomp = zarr.Array(fsstore, partial_decompress=True)\n",
    "load_disk_normally = zarr.Array(fsstore, partial_decompress=False)\n",
    "rand_indices_tuples = (np.random.randint(low=0, high=chunk_shape[0], size=50), np.random.randint(low=0, high=chunk_shape[1], size=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8cc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsstore = zarr.storage.FSStore(\"./fsstore256MBChunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp = zarr.Array(fsstore, partial_decompress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31588943",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp._partial_decompress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk, Not using partial decompression\n",
    "\n",
    "for i in range(50):\n",
    "    load_partial_decomp[rand_indices_tuples[0][i], rand_indices_tuples[1][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0898dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk, Not using partial decompression\n",
    "\n",
    "for i in range(50):\n",
    "    load_disk_normally[rand_indices_tuples[0][i], rand_indices_tuples[1][i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee66fd",
   "metadata": {},
   "source": [
    "## <font color=\"red\">1GB Chunk - write then read</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff5923",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsstore = zarr.storage.FSStore(\"./fsstore1GBChunk\")\n",
    "chunk_shape = (32768,8192)\n",
    "zarr_index_list = zarr.array(numpy_arr, store=fsstore, chunks = chunk_shape)\n",
    "zarr.save(fsstore, zarr_index_list)\n",
    "load_partial_decomp = zarr.Array(fsstore, partial_decompress=True)\n",
    "load_disk_normally = zarr.Array(fsstore, partial_decompress=False)\n",
    "rand_indices_tuples = (np.random.randint(low=0, high=chunk_shape[0], size=50), np.random.randint(low=0, high=chunk_shape[1], size=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk\n",
    "\n",
    "for i in range(50):\n",
    "    load_partial_decomp[rand_indices_tuples[0][i], rand_indices_tuples[1][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from first chunk, Not using partial decompression\n",
    "\n",
    "for i in range(50):\n",
    "    load_disk_normally[rand_indices_tuples[0][i], rand_indices_tuples[1][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xes = []\n",
    "ys = []\n",
    "\n",
    "for i in range(2): # N chunks\n",
    "    xes.append(i*chunk_shape[0]+50)\n",
    "    ys.append(i*chunk_shape[1]+50)\n",
    "    \n",
    "\n",
    "rand_indices_whole_array = list(product(xes, ys))\n",
    "rand_indices_whole_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from whole array, randomly 1 point from each chunk\n",
    "\n",
    "for tup in rand_indices_whole_array:\n",
    "    load_partial_decomp[tup[0], tup[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Read randomly from whole array, randomly 1 point from each chunk\n",
    "\n",
    "for tup in rand_indices_whole_array:\n",
    "    load_disk_normally[tup[0], tup[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032d0a9",
   "metadata": {},
   "source": [
    "### <font color=\"red\">1. Run for longer than 50, see if serializing only once the big chunk, or every time you index it\n",
    "\n",
    "\n",
    "2. Figure out decompression unit size. Experiment: Access ranges: 1K, 4K, 16K, 64K - 1M - 16M\n",
    "</font>\n",
    "\n",
    "3. Can we change compression size? & tradeoffs w/ different sizes\n",
    "\n",
    "\n",
    "3. Crossover betw. one at a time IO and loading whole chunks & indexing points. See how many points you need to index for partial_decompress to be slower\n",
    "\n",
    "\n",
    "4. Mask with sparse, p=0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a6f98",
   "metadata": {},
   "source": [
    "# New Experiments - just read existing data (don't create new FSStores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zarr\n",
    "\n",
    "# Define data location\n",
    "fsstore = zarr.storage.FSStore(\"./fsstore256MBChunk\")\n",
    "# Load data - partial_decompress and full decompress\n",
    "load_partial_decomp = zarr.Array(fsstore, partial_decompress=True)\n",
    "load_disk_normally = zarr.Array(fsstore, partial_decompress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc430ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp._partial_decompress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09968ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620b79f",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Need the above to determine chunk shape. Don't know how to get that programmatically yet</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd003c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_shape = (16384,4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a1430",
   "metadata": {},
   "source": [
    "### Experiment 1: run for longer access (>50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03031e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random indices to test access speed\n",
    "\n",
    "index_lengths = range(50,2000,50) # [50, 100, 150 ... 2000]\n",
    "\n",
    "indices_various_len = []\n",
    "for ind_len in index_lengths:\n",
    "    indices_various_len.append(np.array((np.random.randint(low=0, high=chunk_shape[0], size=ind_len), np.random.randint(low=0, high=chunk_shape[1], size=ind_len))).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bf6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_random_indices_chunk(i, loaded_array):\n",
    "    # Random access of various points in 1 Zarr array chunk\n",
    "    # This shows the cutoff of how many points-per-chunk makes using\n",
    "    # partial_decompress=True advantagous\n",
    "    # Moved to separate function to make %timeit easy\n",
    "    #\n",
    "    # Args:\n",
    "    #    i - how many random accesses to do per chunk (chosen from indices_various_len)\n",
    "    #    loaded_array - zarr array to access elements from\n",
    "    test_indices = indices_various_len[i] # For readability\n",
    "    index_length = indices_various_len[i].shape[0]\n",
    "    \n",
    "    for j in range(index_length):\n",
    "        loaded_array[test_indices[j][0], test_indices[j][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7dc40d",
   "metadata": {},
   "source": [
    "#### partial_decompress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaaeaa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "partial_decomp_chunk_access = []\n",
    "\n",
    "for i in range(len(index_lengths)):\n",
    "    _ = %timeit -o access_random_indices_chunk(i, load_partial_decomp)\n",
    "    partial_decomp_chunk_access.append(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd84e8",
   "metadata": {},
   "source": [
    "#### no partial_decompress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca01751",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decomp_chunk_access = []\n",
    "\n",
    "for i in range(len(index_lengths[:15])): # Stopped at 15/40 since it's so slow\n",
    "    _ = %timeit -o access_random_indices_chunk(i, load_disk_normally)\n",
    "    no_decomp_chunk_access.append(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e9190b",
   "metadata": {},
   "source": [
    "##### Pickle above results to file - let's not lose experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "partial_random_chunk_acc_file = open(\"partial_random_chunk_acc.pickle\", 'wb')\n",
    "normal_random_chunk_acc_file = open(\"normal_random_chunk_acc.pickle\", \"wb\")\n",
    "\n",
    "pickle.dump(partial_decomp_chunk_access, partial_random_chunk_acc_file)\n",
    "pickle.dump(no_decomp_chunk_access, normal_random_chunk_acc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a63343",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_random_chunk_acc_file.close()\n",
    "normal_random_chunk_acc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebc3d7",
   "metadata": {},
   "source": [
    "### Experiment 2 - Stabbing queries\n",
    "\n",
    "<font color=\"red\"> Loading 4MB chunks. 256MB chunk leads to only 8 chunks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zarr\n",
    "\n",
    "# Define data location\n",
    "fsstore = zarr.storage.FSStore(\"./fsstore4MB\")\n",
    "# Load data - partial_decompress and full decompress\n",
    "load_partial_decomp = zarr.Array(fsstore, partial_decompress=True)\n",
    "load_disk_normally = zarr.Array(fsstore, partial_decompress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06443afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp._partial_decompress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8216677",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_shape = (2048, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "xes = []\n",
    "ys = []\n",
    "\n",
    "for i in range(1, 31): # there's 32x32 chunks. TODO get programmatically\n",
    "    # 2d point access - let's not just access first row of chunk\n",
    "    \n",
    "    xes.append(i*int(chunk_shape[0]/2))\n",
    "    ys.append(i*int(chunk_shape[1]/2))\n",
    "    \n",
    "\n",
    "middle_indices_all_chunks = list(product(xes, ys))\n",
    "middle_indices_all_chunks[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dirs = [2**i for i in range(0, 11)] # [-128 : 128] type-access (i.e. either side of center), so multi. by 2\n",
    "y_dirs = [int(x / 4) for x in x_dirs]\n",
    "\n",
    "# Fix y to match x as much as possible. Leave top y-s at 2048 (i.e. whole chunk vertical length / 2)\n",
    "\n",
    "y_dirs[0] = 256\n",
    "y_dirs[1] = 256\n",
    "\n",
    "# x_dirs.append(0)\n",
    "# y_dirs.append(0)\n",
    "\n",
    "x_dirs.sort()\n",
    "y_dirs.sort()\n",
    "print(\"x_dirs: \", x_dirs)\n",
    "print(\"new y_dirs: \", y_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabbing_access(x_stab_size, y_stab_size, target_array):\n",
    "    for tup in middle_indices_all_chunks:\n",
    "#         print([tup[0]-x_stab_size, tup[0]+x_stab_size])\n",
    "        target_array[tup[0]-x_stab_size:tup[0]+x_stab_size, tup[1]-y_stab_size:tup[1]+y_stab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac95a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Stab size of 1 - single point. Complicated indexing prevents me from putting it above\n",
    "\n",
    "for tup in middle_indices_all_chunks:\n",
    "    load_partial_decomp[tup[0], tup[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5f65e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stabbing_partial = [] # only 3 runs\n",
    "\n",
    "for i in range(len(x_dirs)):\n",
    "    _ = %timeit -r 3 -o stabbing_access(x_dirs[i], y_dirs[i], load_partial_decomp)\n",
    "    stabbing_partial.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5fc885",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Stab size of 1 - single point. Complicated indexing prevents me from putting it above\n",
    "\n",
    "for tup in middle_indices_all_chunks:\n",
    "    load_disk_normally[tup[0], tup[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stabbing_normal = [] # only 3 runs\n",
    "\n",
    "for i in range(len(x_dirs)):\n",
    "    _ = %timeit -r 3 -o stabbing_access(x_dirs[i], y_dirs[i], load_disk_normally)\n",
    "    stabbing_normal.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927bb5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_part2 = stabbing_partial.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe5d3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stabbing_partial = [] # only 3 runs\n",
    "\n",
    "# for i in range(len(x_dirs)):\n",
    "_ = %timeit -r 1 -o stabbing_access(1024, 256, load_partial_decomp)\n",
    "stabbing_partial.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed566aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dirs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f641fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "stabbing_partial_file = open(\"stabbing_partial.pickle\", 'wb')\n",
    "stabbing_normal_file = open(\"stabbing_normal.pickle\", \"wb\")\n",
    "\n",
    "pickle.dump(stabbing_partial, stabbing_partial_file)\n",
    "pickle.dump(stabbing_normal, stabbing_normal_file)\n",
    "\n",
    "stabbing_partial_file.close()\n",
    "stabbing_normal_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c0c53",
   "metadata": {},
   "source": [
    "### Experiment 3 - Large Square In-Chunk Access\n",
    "\n",
    "Idea: find how big the \"partial\" decompression piece is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 256MB chunks afresh\n",
    "\n",
    "import numpy as np\n",
    "import zarr\n",
    "\n",
    "# Define data location\n",
    "fsstore = zarr.storage.FSStore(\"./fsstore256MBChunk\")\n",
    "# Load data - partial_decompress and full decompress\n",
    "load_partial_decomp = zarr.Array(fsstore, partial_decompress=True)\n",
    "load_disk_normally = zarr.Array(fsstore, partial_decompress=False)\n",
    "chunk_shape = (16384, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_disk_normally[0, 0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4718069",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_sizes = [2**i for i in range(12, 27)] # 64 KB - 256MB sequential access\n",
    "access_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "[int(a / chunk_shape[1]) for a in access_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50df02d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decomp_sequential_access = []\n",
    "\n",
    "for acc_size in access_sizes:\n",
    "    _ = %timeit -o load_partial_decomp[0:int(acc_size / chunk_shape[1]), 0:4096]\n",
    "    decomp_sequential_access.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decomp_sequential_access = []\n",
    "\n",
    "for acc_size in access_sizes:\n",
    "    _ = %timeit -o load_disk_normally[0:int(acc_size / chunk_shape[1]), 0:4096]\n",
    "    no_decomp_sequential_access.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_disk_normally[0:int(acc_size / chunk_shape[1]), 0:4096].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle above\n",
    "\n",
    "import pickle\n",
    "\n",
    "partial_sequential_chunk_acc_file = open(\"partial_sequential_chunk_acc.pickle\", 'wb')\n",
    "normal_sequential_chunk_acc_file = open(\"normal_sequential_chunk_acc.pickle\", \"wb\")\n",
    "\n",
    "pickle.dump(decomp_sequential_access, partial_sequential_chunk_acc_file)\n",
    "pickle.dump(no_decomp_sequential_access, normal_sequential_chunk_acc_file)\n",
    "\n",
    "partial_sequential_chunk_acc_file.close()\n",
    "normal_sequential_chunk_acc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb515b48",
   "metadata": {},
   "source": [
    "### Experiment 1 Plotting - In-chunk Random access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_partial_means = [x.average for x in partial_decomp_chunk_access]\n",
    "exp1_partial_stds = [x.stdev for x in partial_decomp_chunk_access]\n",
    "\n",
    "exp1_normal_means = [x.average for x in no_decomp_chunk_access]\n",
    "exp1_normal_stds = [x.stdev for x in no_decomp_chunk_access]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59c742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "partial_decomp_chunk_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36273f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decomp_chunk_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.errorbar(index_lengths, exp1_partial_means, yerr=exp1_partial_stds, fmt='.', label=\"Partial\")\n",
    "plt.errorbar(index_lengths[0:15], exp1_normal_means, yerr=exp1_normal_stds, fmt='.', label=\"Full\")\n",
    "\n",
    "plt.xlabel(\"# Points accessed\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"In-chunk Random access (for loop) - 256MB chunks\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b25ef5",
   "metadata": {},
   "source": [
    "## Same experiment, multiple chunk sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f7b0e",
   "metadata": {},
   "source": [
    "#### Access 64KB - 1GB on arrays chunked in various sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1555bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 4MB chunks afresh\n",
    "\n",
    "import numpy as np\n",
    "import zarr\n",
    "\n",
    "# Define data location\n",
    "fsstore = zarr.storage.FSStore(\"./fsstore4MB\")\n",
    "# Load data - partial_decompress and full decompress\n",
    "load_partial_decomp = zarr.Array(fsstore, partial_decompress=True)\n",
    "load_disk_normally = zarr.Array(fsstore, partial_decompress=False)\n",
    "chunk_shape = (2048, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ab5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_disk_normally.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5526939",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_disk_normally[0, 0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e077d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_sizes = [2**i for i in range(12, 21)] # 64 KB - 4MB sequential access. Remember int32-4bytes\n",
    "access_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee425e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[int(a / chunk_shape[1]) for a in access_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a920b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_sequential_access = []\n",
    "\n",
    "for acc_size in access_sizes:\n",
    "    _ = %timeit -o load_partial_decomp[0:int(acc_size / chunk_shape[1]), 0:512]\n",
    "    decomp_sequential_access.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decomp_sequential_access = []\n",
    "\n",
    "for acc_size in access_sizes:\n",
    "    _ = %timeit -o load_disk_normally[0:int(acc_size / chunk_shape[1]), 0:512]\n",
    "    no_decomp_sequential_access.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c127a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle above\n",
    "\n",
    "import pickle\n",
    "\n",
    "partial_sequential_chunk_acc_file = open(\"partial_sequential_4MBchunk_acc.pickle\", 'wb')\n",
    "normal_sequential_chunk_acc_file = open(\"normal_sequential_4MBchunk_acc.pickle\", \"wb\")\n",
    "\n",
    "pickle.dump(decomp_sequential_access, partial_sequential_chunk_acc_file)\n",
    "pickle.dump(no_decomp_sequential_access, normal_sequential_chunk_acc_file)\n",
    "\n",
    "partial_sequential_chunk_acc_file.close()\n",
    "normal_sequential_chunk_acc_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.errorbar(index_lengths, exp1_partial_means, yerr=exp1_partial_stds, fmt='.', label=\"Partial\")\n",
    "plt.errorbar(index_lengths[0:15], exp1_normal_means, yerr=exp1_normal_stds, fmt='.', label=\"Full\")\n",
    "\n",
    "plt.xlabel(\"# Points accessed\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"In-chunk Random access (for loop) - 256MB chunks\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496720f3",
   "metadata": {},
   "source": [
    "### Experiment 2 Plotting - Stabbing across-chunk access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364bd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_partial_means = [x.average for x in stabbing_partial]\n",
    "exp2_partial_stds = [x.stdev for x in stabbing_partial]\n",
    "\n",
    "exp2_normal_means = [x.average for x in stabbing_normal]\n",
    "exp2_normal_stds = [x.stdev for x in stabbing_normal]\n",
    "\n",
    "# Can't label x-axis with tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_points_accessed = []\n",
    "\n",
    "for i in range(len(x_dirs)):\n",
    "    exp2_points_accessed.append(2*x_dirs[i] + 2*y_dirs[i])\n",
    "\n",
    "exp2_points_accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "stabbing_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "stabbing_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72250551",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_points_accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93072955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.errorbar(exp2_points_accessed[0:10], exp2_partial_means, yerr=exp2_partial_stds, fmt='.', label=\"Partial\")\n",
    "plt.errorbar(exp2_points_accessed, exp2_normal_means, yerr=exp2_normal_stds, fmt='.', label=\"Full\")\n",
    "\n",
    "plt.xlabel(\"# Points accessed\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Stabbing across-chunk access - 4MB chunks\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6c84c",
   "metadata": {},
   "source": [
    "### Experiment 3 Plotting - Large Square In-Chunk access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_partial_means = [x.average for x in decomp_sequential_access]\n",
    "exp3_partial_stds = [x.stdev for x in decomp_sequential_access]\n",
    "\n",
    "exp3_normal_means = [x.average for x in no_decomp_sequential_access]\n",
    "exp3_normal_stds = [x.stdev for x in no_decomp_sequential_access]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc987f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_sequential_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa1715",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decomp_sequential_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ed47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0dae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.errorbar(access_sizes, exp3_partial_means, yerr=exp3_partial_stds, fmt='.', label=\"Partial\")\n",
    "plt.errorbar(access_sizes, exp3_normal_means, yerr=exp3_normal_stds, fmt='.', label=\"Full\")\n",
    "\n",
    "plt.xlabel(\"# Points accessed\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Large Square In-Chunk access - 256MB chunks\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff2a72",
   "metadata": {},
   "source": [
    "## Experiment 4 - Lagrangian 3D point access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_3d_arr = np.random.randint(low=0, high=1000, size=(16384, 4096, 80), dtype=np.int32) # 100 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.info(numpy_3d_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36582241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fails bcs. it runs out of memory. Cannot define dtype manually\n",
    "# numpy_3d_arr = np.random.random(size=(65536, 16384, 25), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e3b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "fsstore = zarr.storage.FSStore('./fsstore3D')\n",
    "zarr_index_list = zarr.array(numpy_3d_arr, store=fsstore)#, chunks=chunk_shape)\n",
    "zarr.save(fsstore, numpy_3d_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093939d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del numpy_3d_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 4MB chunks afresh\n",
    "\n",
    "import numpy as np\n",
    "import zarr\n",
    "\n",
    "# Define data location\n",
    "fsstore = zarr.storage.FSStore(\"./fsstore3D\")\n",
    "# Load data - partial_decompress and full decompress\n",
    "load_partial_decomp = zarr.Array(fsstore, partial_decompress=True)\n",
    "load_disk_normally = zarr.Array(fsstore, partial_decompress=False)\n",
    "# chunk_shape = (2048, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1514cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_partial_decomp.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0181a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
